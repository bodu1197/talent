#!/usr/bin/env node

/**
 * Supabase í”„ë¡œì íŠ¸ ì „ì²´ ì„¤ì • export ìŠ¤í¬ë¦½íŠ¸
 *
 * Export í•­ëª©:
 * 1. í”„ë¡œì íŠ¸ ê¸°ë³¸ ì •ë³´
 * 2. Auth ì„¤ì • (providers, policies)
 * 3. Storage buckets ë° policies
 * 4. Database ì„¤ì •
 * 5. Edge Functions
 * 6. Secrets & Environment Variables
 */

const https = require('https');
const fs = require('fs');
const path = require('path');

const SUPABASE_PROJECT_ID = 'bpvfkkrlyrjkwgwmfrci';
const SUPABASE_ACCESS_TOKEN = 'sbp_140ed0f35c7b31aa67f56bdca11db02fd469802f';
const OUTPUT_DIR = path.join(__dirname, '..', 'supabase-settings-export');

// Output ë””ë ‰í† ë¦¬ ìƒì„±
if (!fs.existsSync(OUTPUT_DIR)) {
  fs.mkdirSync(OUTPUT_DIR, { recursive: true });
}

function apiRequest(path, method = 'GET', body = null) {
  return new Promise((resolve, reject) => {
    const options = {
      hostname: 'api.supabase.com',
      path: path,
      method: method,
      headers: {
        'Authorization': `Bearer ${SUPABASE_ACCESS_TOKEN}`,
        'Content-Type': 'application/json'
      }
    };

    const req = https.request(options, (res) => {
      let data = '';

      res.on('data', (chunk) => {
        data += chunk;
      });

      res.on('end', () => {
        if (res.statusCode >= 200 && res.statusCode < 300) {
          try {
            resolve(data ? JSON.parse(data) : {});
          } catch (error) {
            console.error('ì—ëŸ¬ ë°œìƒ:', error);
            resolve(data); // Return raw data if not JSON
          }
        } else {
          reject(new Error(`HTTP ${res.statusCode}: ${data}`));
        }
      });
    });

    req.on('error', reject);
    if (body) {
      req.write(JSON.stringify(body));
    }
    req.end();
  });
}

function executeQuery(query) {
  return new Promise((resolve, reject) => {
    const data = JSON.stringify({ query });

    const options = {
      hostname: 'api.supabase.com',
      path: `/v1/projects/${SUPABASE_PROJECT_ID}/database/query`,
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${SUPABASE_ACCESS_TOKEN}`,
        'Content-Type': 'application/json',
        'Content-Length': data.length
      }
    };

    const req = https.request(options, (res) => {
      let body = '';

      res.on('data', (chunk) => {
        body += chunk;
      });

      res.on('end', () => {
        if (res.statusCode === 200 || res.statusCode === 201) {
          try {
            resolve(JSON.parse(body));
          } catch (error) {
            console.error('ì—ëŸ¬ ë°œìƒ:', error);
            reject(new Error(`Failed to parse response: ${e.message}`));
          }
        } else {
          reject(new Error(`HTTP ${res.statusCode}: ${body}`));
        }
      });
    });

    req.on('error', reject);
    req.write(data);
    req.end();
  });
}

async function exportProjectInfo() {
  console.log('ğŸ“‹ Exporting project information...');

  try {
    const project = await apiRequest(`/v1/projects/${SUPABASE_PROJECT_ID}`);

    const outputPath = path.join(OUTPUT_DIR, 'project-info.json');
    fs.writeFileSync(outputPath, JSON.stringify(project, null, 2));

    console.log('âœ… Project information exported\n');
    return project;
  } catch (error) {
    console.error(`âŒ Error: ${error.message}\n`);
    return null;
  }
}

async function exportAuthSettings() {
  console.log('ğŸ“‹ Exporting Auth settings...');

  try {
    // Auth providers ì„¤ì •
    const config = await apiRequest(`/v1/projects/${SUPABASE_PROJECT_ID}/config`);

    const outputPath = path.join(OUTPUT_DIR, 'auth-config.json');
    fs.writeFileSync(outputPath, JSON.stringify(config, null, 2));

    console.log('âœ… Auth settings exported\n');
  } catch (error) {
    console.error(`âŒ Error: ${error.message}\n`);
  }
}

async function exportStorageBuckets() {
  console.log('ğŸ“‹ Exporting Storage buckets...');

  try {
    // Storage buckets ëª©ë¡
    const buckets = await executeQuery(`
      SELECT * FROM storage.buckets
    `);

    const bucketsPath = path.join(OUTPUT_DIR, 'storage-buckets.json');
    fs.writeFileSync(bucketsPath, JSON.stringify(buckets, null, 2));

    console.log(`âœ… Exported ${buckets.length} storage buckets\n`);

    // Storage policies
    console.log('ğŸ“‹ Exporting Storage policies...');
    const policies = await executeQuery(`
      SELECT
        schemaname,
        tablename,
        policyname,
        permissive,
        roles,
        cmd,
        qual,
        with_check
      FROM pg_policies
      WHERE schemaname = 'storage'
      ORDER BY tablename, policyname
    `);

    const policiesPath = path.join(OUTPUT_DIR, 'storage-policies.json');
    fs.writeFileSync(policiesPath, JSON.stringify(policies, null, 2));

    console.log(`âœ… Exported ${policies.length} storage policies\n`);
  } catch (error) {
    console.error(`âŒ Error: ${error.message}\n`);
  }
}

async function exportRLSPolicies() {
  console.log('ğŸ“‹ Exporting RLS policies...');

  try {
    const policies = await executeQuery(`
      SELECT
        schemaname,
        tablename,
        policyname,
        permissive,
        roles,
        cmd,
        qual,
        with_check
      FROM pg_policies
      WHERE schemaname = 'public'
      ORDER BY tablename, policyname
    `);

    const outputPath = path.join(OUTPUT_DIR, 'rls-policies.json');
    fs.writeFileSync(outputPath, JSON.stringify(policies, null, 2));

    console.log(`âœ… Exported ${policies.length} RLS policies\n`);
  } catch (error) {
    console.error(`âŒ Error: ${error.message}\n`);
  }
}

async function exportFunctions() {
  console.log('ğŸ“‹ Exporting database functions...');

  try {
    const functions = await executeQuery(`
      SELECT
        n.nspname as schema,
        p.proname as name,
        pg_get_function_arguments(p.oid) as arguments,
        pg_get_functiondef(p.oid) as definition,
        CASE p.provolatile
          WHEN 'i' THEN 'IMMUTABLE'
          WHEN 's' THEN 'STABLE'
          WHEN 'v' THEN 'VOLATILE'
        END as volatility,
        CASE p.prosecdef
          WHEN true THEN 'SECURITY DEFINER'
          ELSE 'SECURITY INVOKER'
        END as security
      FROM pg_proc p
      JOIN pg_namespace n ON p.pronamespace = n.oid
      WHERE n.nspname NOT IN ('pg_catalog', 'information_schema')
      AND p.prokind = 'f'
      ORDER BY n.nspname, p.proname
    `);

    const outputPath = path.join(OUTPUT_DIR, 'database-functions.json');
    fs.writeFileSync(outputPath, JSON.stringify(functions, null, 2));

    // SQL í˜•íƒœë¡œë„ ì €ì¥
    const sqlPath = path.join(OUTPUT_DIR, 'database-functions.sql');
    const sqlContent = functions.map(f => f.definition).join('\n\n');
    fs.writeFileSync(sqlPath, sqlContent);

    console.log(`âœ… Exported ${functions.length} database functions\n`);
  } catch (error) {
    console.error(`âŒ Error: ${error.message}\n`);
  }
}

async function exportTriggers() {
  console.log('ğŸ“‹ Exporting triggers...');

  try {
    const triggers = await executeQuery(`
      SELECT
        trigger_schema,
        trigger_name,
        event_manipulation,
        event_object_table,
        action_statement,
        action_timing,
        action_orientation
      FROM information_schema.triggers
      WHERE trigger_schema = 'public'
      ORDER BY event_object_table, trigger_name
    `);

    const outputPath = path.join(OUTPUT_DIR, 'triggers.json');
    fs.writeFileSync(outputPath, JSON.stringify(triggers, null, 2));

    console.log(`âœ… Exported ${triggers.length} triggers\n`);
  } catch (error) {
    console.error(`âŒ Error: ${error.message}\n`);
  }
}

async function exportIndexes() {
  console.log('ğŸ“‹ Exporting indexes...');

  try {
    const indexes = await executeQuery(`
      SELECT
        schemaname,
        tablename,
        indexname,
        indexdef
      FROM pg_indexes
      WHERE schemaname = 'public'
      ORDER BY tablename, indexname
    `);

    const outputPath = path.join(OUTPUT_DIR, 'indexes.json');
    fs.writeFileSync(outputPath, JSON.stringify(indexes, null, 2));

    // SQL í˜•íƒœë¡œë„ ì €ì¥
    const sqlPath = path.join(OUTPUT_DIR, 'indexes.sql');
    const sqlContent = indexes.map(idx => idx.indexdef + ';').join('\n');
    fs.writeFileSync(sqlPath, sqlContent);

    console.log(`âœ… Exported ${indexes.length} indexes\n`);
  } catch (error) {
    console.error(`âŒ Error: ${error.message}\n`);
  }
}

async function exportViews() {
  console.log('ğŸ“‹ Exporting views...');

  try {
    const views = await executeQuery(`
      SELECT
        table_schema,
        table_name,
        view_definition
      FROM information_schema.views
      WHERE table_schema = 'public'
      ORDER BY table_name
    `);

    const outputPath = path.join(OUTPUT_DIR, 'views.json');
    fs.writeFileSync(outputPath, JSON.stringify(views, null, 2));

    // SQL í˜•íƒœë¡œë„ ì €ì¥
    const sqlPath = path.join(OUTPUT_DIR, 'views.sql');
    const sqlContent = views.map(v =>
      `CREATE OR REPLACE VIEW ${v.table_name} AS\n${v.view_definition};`
    ).join('\n\n');
    fs.writeFileSync(sqlPath, sqlContent);

    console.log(`âœ… Exported ${views.length} views\n`);
  } catch (error) {
    console.error(`âŒ Error: ${error.message}\n`);
  }
}

async function exportEnums() {
  console.log('ğŸ“‹ Exporting enum types...');

  try {
    const enums = await executeQuery(`
      SELECT
        t.typname as enum_name,
        array_agg(e.enumlabel ORDER BY e.enumsortorder) as enum_values
      FROM pg_type t
      JOIN pg_enum e ON t.oid = e.enumtypid
      JOIN pg_namespace n ON n.oid = t.typnamespace
      WHERE n.nspname = 'public'
      GROUP BY t.typname
      ORDER BY t.typname
    `);

    const outputPath = path.join(OUTPUT_DIR, 'enums.json');
    fs.writeFileSync(outputPath, JSON.stringify(enums, null, 2));

    // SQL í˜•íƒœë¡œë„ ì €ì¥
    const sqlPath = path.join(OUTPUT_DIR, 'enums.sql');
    const sqlContent = enums.map(e => {
      const enumValues = e.enum_values.map(v => `'${v}'`).join(', ');
      return `CREATE TYPE ${e.enum_name} AS ENUM (${enumValues});`;
    }).join('\n');
    fs.writeFileSync(sqlPath, sqlContent);

    console.log(`âœ… Exported ${enums.length} enum types\n`);
  } catch (error) {
    console.error(`âŒ Error: ${error.message}\n`);
  }
}

async function createImportGuide() {
  console.log('ğŸ“‹ Creating import guide...');

  const guide = `# Supabase í”„ë¡œì íŠ¸ Import ê°€ì´ë“œ

## 1. ìƒˆ Supabase í”„ë¡œì íŠ¸ ìƒì„±

1. https://supabase.com/dashboard ì ‘ì†
2. "New Project" í´ë¦­
3. í”„ë¡œì íŠ¸ ì´ë¦„, ë¹„ë°€ë²ˆí˜¸, ë¦¬ì „ ì„¤ì •
4. í”„ë¡œì íŠ¸ ìƒì„± ì™„ë£Œ ëŒ€ê¸°

## 2. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ Import

### ë°©ë²• 1: Migration íŒŒì¼ ì‚¬ìš© (ê¶Œì¥)

\`\`\`bash
# Supabase CLI ì„¤ì¹˜
npm install -g supabase

# í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
supabase init

# ê¸°ì¡´ migration íŒŒì¼ë“¤ì„ ìƒˆ í”„ë¡œì íŠ¸ì˜ supabase/migrations/ í´ë”ë¡œ ë³µì‚¬
# (ì´ í”„ë¡œì íŠ¸ì˜ supabase/migrations/ í´ë” ì „ì²´)

# ìƒˆ Supabase í”„ë¡œì íŠ¸ ì—°ê²°
supabase link --project-ref [NEW_PROJECT_ID]

# Migration ì‹¤í–‰
supabase db push
\`\`\`

### ë°©ë²• 2: SQL íŒŒì¼ ì§ì ‘ ì‹¤í–‰

1. Supabase Dashboard â†’ SQL Editor
2. ë‹¤ìŒ ìˆœì„œë¡œ SQL íŒŒì¼ ì‹¤í–‰:
   - \`enums.sql\` (ë¨¼ì € ì‹¤í–‰)
   - \`database-export/full-schema.sql\`
   - \`indexes.sql\`
   - \`views.sql\`
   - \`database-functions.sql\`

## 3. RLS Policies Import

Dashboard â†’ Authentication â†’ Policiesì—ì„œ:
- \`rls-policies.json\` ë‚´ìš© ì°¸ê³ í•˜ì—¬ ì •ì±… ìˆ˜ë™ ìƒì„±
- ë˜ëŠ” SQL Editorì—ì„œ CREATE POLICY ë¬¸ ì‹¤í–‰

## 4. Storage ì„¤ì •

### Buckets ìƒì„±
Dashboard â†’ Storage â†’ Create bucket
- \`storage-buckets.json\` ì°¸ê³ 

### Storage Policies ì„¤ì •
Dashboard â†’ Storage â†’ [Bucket] â†’ Policies
- \`storage-policies.json\` ì°¸ê³ 

## 5. Auth ì„¤ì •

Dashboard â†’ Authentication â†’ Providers
- \`auth-config.json\` ì°¸ê³ í•˜ì—¬ ì„¤ì •

## 6. ë°ì´í„° Import

### ë°©ë²• 1: JSON ë°ì´í„° Import ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©

\`\`\`bash
node scripts/import-data.js
\`\`\`

### ë°©ë²• 2: SQL INSERT ë¬¸ ìƒì„± í›„ ì‹¤í–‰

ê° \`.json\` íŒŒì¼ì„ INSERT ë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì‹¤í–‰

## 7. Environment Variables ì—…ë°ì´íŠ¸

ìƒˆ í”„ë¡œì íŠ¸ì˜ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸:
- \`NEXT_PUBLIC_SUPABASE_URL\`
- \`NEXT_PUBLIC_SUPABASE_ANON_KEY\`
- \`SUPABASE_SERVICE_ROLE_KEY\`

## 8. ê²€ì¦

1. ëª¨ë“  í…Œì´ë¸”ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
2. ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ importë˜ì—ˆëŠ”ì§€ í™•ì¸
3. RLS policiesê°€ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
4. Storage ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
5. Auth ë¡œê·¸ì¸/íšŒì›ê°€ì… í…ŒìŠ¤íŠ¸

## Exportëœ íŒŒì¼ ëª©ë¡

### ë°ì´í„°ë² ì´ìŠ¤
- \`database-export/\`: ëª¨ë“  í…Œì´ë¸” ë°ì´í„° (JSON)
- \`full-schema.sql\`: ì „ì²´ ìŠ¤í‚¤ë§ˆ
- \`foreign-keys.json\`: ì™¸ë˜ í‚¤ ì œì•½ì¡°ê±´

### ë°ì´í„°ë² ì´ìŠ¤ ê°ì²´
- \`enums.sql\`: Enum íƒ€ì…
- \`indexes.sql\`: ì¸ë±ìŠ¤
- \`views.sql\`: ë·°
- \`database-functions.sql\`: í•¨ìˆ˜
- \`triggers.json\`: íŠ¸ë¦¬ê±°

### ì„¤ì •
- \`project-info.json\`: í”„ë¡œì íŠ¸ ê¸°ë³¸ ì •ë³´
- \`auth-config.json\`: Auth ì„¤ì •
- \`rls-policies.json\`: RLS ì •ì±…
- \`storage-buckets.json\`: Storage ë²„í‚·
- \`storage-policies.json\`: Storage ì •ì±…

### Migration íŒŒì¼
- \`supabase/migrations/\`: ê¸°ì¡´ migration íŒŒì¼ (50ê°œ)

## ì£¼ì˜ì‚¬í•­

1. **ìˆœì„œ ì¤‘ìš”**: Enum â†’ Schema â†’ Indexes â†’ Views â†’ Functions ìˆœì„œë¡œ import
2. **ì™¸ë˜ í‚¤**: ì°¸ì¡°ë˜ëŠ” í…Œì´ë¸”ì„ ë¨¼ì € import
3. **RLS**: ë°ì´í„° import ì „ì— RLSë¥¼ ë¹„í™œì„±í™”í•˜ê³ , import í›„ ë‹¤ì‹œ í™œì„±í™”
4. **Secrets**: API keys, passwordsëŠ” ìˆ˜ë™ìœ¼ë¡œ ì„¤ì • í•„ìš”
5. **Storage**: íŒŒì¼ì€ ë³„ë„ë¡œ ë‹¤ìš´ë¡œë“œ/ì—…ë¡œë“œ í•„ìš”

## ë„ì›€ì´ í•„ìš”í•œ ê²½ìš°

- Supabase ê³µì‹ ë¬¸ì„œ: https://supabase.com/docs
- CLI ì°¸ê³ : https://supabase.com/docs/reference/cli
`;

  const guidePath = path.join(OUTPUT_DIR, 'IMPORT-GUIDE.md');
  fs.writeFileSync(guidePath, guide);

  console.log('âœ… Import guide created\n');
}

async function main() {
  console.log('ğŸš€ Starting Supabase settings export...\n');
  console.log(`Output directory: ${OUTPUT_DIR}\n`);

  try {
    await exportProjectInfo();
    await exportAuthSettings();
    await exportStorageBuckets();
    await exportRLSPolicies();
    await exportFunctions();
    await exportTriggers();
    await exportIndexes();
    await exportViews();
    await exportEnums();
    await createImportGuide();

    console.log('âœ… Export completed successfully!');
    console.log(`\nğŸ“ Files saved to: ${OUTPUT_DIR}`);
    console.log('\nğŸ“– Read IMPORT-GUIDE.md for instructions on how to import to a new project');
  } catch (error) {
    console.error('âŒ Export failed:', error);
    process.exit(1);
  }
}

main();
